# CNN-accelrator
RTL + ML hybrid project that bridges the hardware and software components

Hybrid RTL + ML: Hardware-Accelerated Convolution Engine
This project demonstrates the design, verification, and implementation of a modular CNN (Convolutional Neural Network) accelerator using Verilog for the hardware component and Python with TensorFlow for data preparation and verification.

The core of the project is a pipelined RTL design that performs a 3x3 convolution, a fundamental operation in CNNs. A Python script is used to extract real-world data from the MNIST dataset, quantize it, and generate the necessary memory files (.mem) for simulation. The script also calculates a "golden reference" value, which is used to verify the functional correctness of the RTL design.

Project Status: ‚úÖ Verification Complete

Synthesis: Successful on a Xilinx 7-series FPGA (xc7k70tfbv676-1).

Simulation: The Verilog testbench output perfectly matches the golden reference generated by the Python script, confirming the accelerator's logic is correct.

**üõ†Ô∏è Hardware Design (RTL)**

The CNN accelerator is designed in Verilog with a modular and pipelined architecture to ensure scalability and efficient hardware implementation.

Modules
  - cnn_accel_top: The top-level wrapper that integrates all sub-modules, handles the clock buffering (BUFG), and exposes the primary I/O ports (clk_in, rst, start, done, result_out).

  - layer_fsm: A finite state machine that controls the operation of the accelerator. It orchestrates the data loading, convolution, and result writing stages.

  - mem_interface: Simulates memory by reading the feature map and kernel data from fmap.mem and kernel.mem using $readmemb. It presents this data to the convolution engine.

  - conv_engine: The computational core of the accelerator. It instantiates nine mac_unit modules and contains a combinational adder tree to sum their results, performing the 3x3 convolution in a single cycle.

  - mac_unit: A registered Multiply-Accumulate unit that multiplies an 8-bit feature map pixel by an 8-bit kernel weight.

  - output_buffer: A simple register that latches the final 20-bit result from the convolution engine.

  - cnn_accel_tb: The testbench that instantiates the cnn_accel_top, provides the clock and reset signals, and monitors the done and result_out signals for verification.

**üêç Software Component (Python)**

The Python script generate_test_data.py is essential for creating a realistic test case and verifying the RTL design.

Script Functionality

  - Load Data: It loads the standard MNIST dataset using TensorFlow/Keras.

  - Extract Data: It takes a single 28x28 image and extracts a 3x3 slice to serve as the feature map. It also defines a sample 3x3 floating-point kernel.

  - Quantization: It converts the 32-bit floating-point feature map and kernel data into 8-bit unsigned integers (uint8). This critical step mimics how data is handled in resource-constrained hardware accelerators.

  - Feature map values [0.0, 1.0] are scaled to [0, 255].

  - Kernel weights [-1.0, 1.0] are first scaled to [0.0, 1.0] and then to [0, 255].

  - Generate .mem Files: It writes the quantized, flattened data into fmap.mem and kernel.mem as binary strings. This format is robustly parsed by Vivado's $readmemb system task.

  - Calculate Golden Reference: It performs the convolution calculation using NumPy, carefully casting the integer values to a wider type before multiplication to prevent overflow. This result serves as the ground truth for verifying the RTL simulation.

üöÄ How to Run and Verify

Dependencies

    - Hardware: Vivado Design Suite (2018.1 or later)

    - Software: Python 3.x, NumPy, TensorFlow

Step 1: Generate Test Data

  - First, run the Python script to create the memory files and find the expected result.

  - Navigate to the scripts directory
    
    - cd scripts

  - Run the script
    - python generate_test_data.py

  - This will create fmap.mem and kernel.mem in the root directory and print the Golden Reference Result to the console. Take note of this value.

  - Step 2: Run the RTL Simulation
    - Create a new project in Vivado.

    - Add all the Verilog files from the rtl/ and sim/ directories.

    - Ensure fmap.mem and kernel.mem are in a location accessible to the simulation (e.g., the project's sim_1 directory).

    - Set cnn_accel_tb as the top simulation module.

    - Run the behavioral simulation.

  - Step 3: Verify the Output
    - Once the simulation completes, observe the waveform or console output. The testbench will wait for the done signal to go high and then display the result_out from the accelerator.

# This result_out value from the Vivado simulation should be identical to the "Golden Reference Result" printed by the Python script.
